\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The overview of the proposed framework for gait recognition. 2D human poses were first extracted from raw video frames using improved OpenPose\nobreakspace {}\cite {Cao_19} algorithm. Effective body joints were then selected from each subject pose estimation and a timestep of 28 frame pose sequence was formed to feed into a temporal network. The temporal network identified the subject by modeling the gait features. \relax }}{9}{figure.caption.4}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Different features extraction process of the proposed method. a) 6 effective joints were selected out of 25 body joints as estimated from pose estimation algorithm\nobreakspace {}\cite {Cao_19} to form a 12-dimensional pose vector. b) 5 angular trajectories from lower limbs were considered to form a joint-angle feature vector. c) A total of 8 body joints were selected to get temporal displacement features. d) 7 body parts were taken to form a limb length feature vector. \relax }}{11}{figure.caption.5}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Examples of 2D human pose estimation by\nobreakspace {}\cite {Cao_19} from RGB images of CASIA dataset (left ones). Detected 25 human body joints with description are shown. (right ones) \relax }}{13}{figure.caption.7}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Proposed RNN architecture for robust gait recognition. It consists of two BiGRU\nobreakspace {}\cite {Schuster_97} layers each of which consists of 80 GRU cells with one batch normalization and one output softmax layer. The network was fed with 50 dimensional input from 2D pose estimation. Input layer was followed by a batch normalization layer\nobreakspace {}\cite {Ioffe_15}. The output of recurrent layers was also batch normalized to standardize the activations and finally fed into an output softmax layer. For the output layer, the number of the output neuron equals to the number of subjects. \relax }}{16}{figure.caption.8}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Output prediction scheme of our proposed temporal network. Each input clip was considered as a separate video and a sequence of class probabilities was predicted at output. Majority voting scheme was used to process the output to predict the subject ID. \relax }}{18}{figure.caption.10}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Overview of our proposed multi-view gait recognition network scheme. YOLOv3\nobreakspace {}\cite {Redmon_18} was used to detect and locate the walking people in video frames. The input of the network was a clip of 16 consecutive frame which was preprocessed and resized to $112\times 112$ to feed into a 3D convolutional network based on C3D\nobreakspace {}\cite {Tran_15}. The network used 3D kernels to exploit spatio-temporal dynamics for viewing angle identification. Thereafter, a temporal network, trained on each viewing angle, was performed subject identification by modeling temporal dynamics from input 2D pose sequence \relax }}{19}{figure.caption.11}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Proposed 3D-CNN for video angle identification. Last 3 layers of a pretrained C3D\nobreakspace {}\cite {Tran_15} network has been replaced by a fully connected layer of 128 neurons followed a final softmax layer of 11 neurons to classify 11 different walking direction in CASIA-B dataset. \relax }}{20}{figure.caption.12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Sample video frames of CASIA A and CASIA B dataset. In top, some of the sample images from CASIA A dataset are shown where subjects walking along straight line in 3 different view angle, and in bottom, CASIA B dataset is shown with its 11 viewing angle. \relax }}{23}{figure.caption.14}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison in correct class recognition rate (CCR) at different viewing angles among proposed method with other prevailing gait recognition methods proposed in literature on CASIA A dataset. Our method achieves $ 100\% $ class recognition rate on all of the view angles which proved the efficacy of the proposed method. \relax }}{25}{figure.caption.16}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Correct class recognition rates (\%) of the proposed method with other state-of-the-art methods on all three probe set of CASIA-B dataset without view variation. Proposed method demonstrates better performance compared to other by achieving $89.64\%$ and $96.45\%$ in two covariate conditions of CASIA-B dataset \textit {ProbeCL}, and \textit {ProbeBG} respectively. The result proves the robustness of proposed pose-based temporal network against carrying and clothing conditions variations. \relax }}{27}{figure.caption.23}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison with different state-of-the-art methods for gait recognition with view variation in all three probe set of CASIA B dataset. Here, the value reported for each algorithm is the average of all the gallery view's average CCR. Proposed method outperforms other state-of-the-art methods achieving $47.23\%$ and $33.46\%$ in two covariate conditions \textit {ProbeBG}, and \textit {ProbeCL} respectively. \relax }}{28}{figure.caption.28}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Average recognition rates(\%) of the proposed method compared to the other state-of-the-art methods in multi-view gait recognition. Proposed method achieves higher average recognition accuracy on 8 of total 11 probe angles of CASIA-B dataset compared other methods in literature. \relax }}{31}{figure.caption.32}
\addvspace {10\p@ }
\addvspace {10\p@ }
