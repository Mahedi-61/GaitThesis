\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{gait_thesis.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Certification}}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Candidate's Declaration}}{ii}{DTLrowi.1.5}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Abstract}}{iii}{DTLrowi.2.1}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Acknowledgement}}{iv}{DTLrowi.2.1}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Redmon_18}
\citation{Tran_15}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{section*.1}}
\citation{Tran_15}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{section*.2}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {List of Abbreviations}}{xii}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\newlabel{intro@cref}{{[chapter][1][]1}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Gait and Gait Recognition}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Challenges in Gait Recognition}{2}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Problem Definition}{2}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Objectives of the Thesis}{3}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Overview of the Thesis}{3}{section.1.5}}
\citation{Yu_06}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Contributions of the Thesis}{4}{section.1.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Thesis Outline}{5}{section.1.7}}
\citation{Rida_19}
\citation{Han_06,Bashir_09,Lam_11}
\citation{Han_06}
\citation{Bashir_09}
\citation{Lam_11}
\citation{Huang_12}
\citation{Chao_19}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{6}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{literature_review}{{2}{6}{Literature Review}{chapter.2}{}}
\newlabel{literature_review@cref}{{[chapter][2][]2}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Appearance-based Methods}{6}{section.2.1}}
\citation{Yam_04,Ariyanto_11,Tafazzoli_10,Feng_16}
\citation{Yam_04}
\citation{Ariyanto_11}
\citation{Tafazzoli_10}
\citation{Wu_17,Shiraga_16,Wolf_16,Zhang_16,Yu_17,Yu_19}
\citation{Wu_17}
\citation{Shiraga_16}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Model-based Methods}{7}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning for Gait Recognition}{7}{section.2.3}}
\citation{Wolf_16}
\citation{Zhang_16}
\citation{Yu_17}
\citation{Yu_19}
\citation{Cao_19}
\citation{Feng_16,Liao_17,Liao_19}
\citation{Feng_16}
\citation{Liao_17}
\citation{Liao_19}
\citation{Song_17,Du_15}
\citation{Song_17}
\citation{Du_15}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Pose-based Gait Recognition}{8}{section.2.4}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cunado_97}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{10}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{methodology}{{3}{10}{Methodology}{chapter.3}{}}
\newlabel{methodology@cref}{{[chapter][3][]3}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{10}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Forming Spatio-Temporal Features}{10}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}2D Body Joints Feature}{10}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  The overview of the proposed framework for gait recognition. 2D human poses were first extracted from raw video frames using improved OpenPose\nobreakspace  {}\cite  {Cao_19} algorithm. Effective body joints were then selected from each subject pose estimation and a timestep of 28 frame pose sequence was formed to feed into a temporal network. The temporal network identified the subject by modeling the gait features. \relax }}{11}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:proposed_method}{{3.1}{11}{The overview of the proposed framework for gait recognition. 2D human poses were first extracted from raw video frames using improved OpenPose~\cite {Cao_19} algorithm. Effective body joints were then selected from each subject pose estimation and a timestep of 28 frame pose sequence was formed to feed into a temporal network. The temporal network identified the subject by modeling the gait features. \relax }{figure.caption.5}{}}
\newlabel{fig:proposed_method@cref}{{[figure][1][3]3.1}{[1][10][]11}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Wang_04}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Joints Angular Trajectory}{12}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Different features extraction process of the proposed method. a) 6 effective joints were selected out of 25 body joints as estimated from pose estimation algorithm\nobreakspace  {}\cite  {Cao_19} to form a 12-dimensional pose vector. b) 5 angular trajectories from lower limbs were considered to form a joint-angle feature vector. c) A total of 8 body joints were selected to get temporal displacement features. d) 7 body parts were taken to form a limb length feature vector. \relax }}{13}{figure.caption.6}}
\newlabel{fig:extracted_features}{{3.2}{13}{Different features extraction process of the proposed method. a) 6 effective joints were selected out of 25 body joints as estimated from pose estimation algorithm~\cite {Cao_19} to form a 12-dimensional pose vector. b) 5 angular trajectories from lower limbs were considered to form a joint-angle feature vector. c) A total of 8 body joints were selected to get temporal displacement features. d) 7 body parts were taken to form a limb length feature vector. \relax }{figure.caption.6}{}}
\newlabel{fig:extracted_features@cref}{{[figure][2][3]3.2}{[1][12][]13}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces List of effective joint-angle trajectories with corresponding body joints set for gait angular feature vector \relax }}{13}{table.caption.7}}
\newlabel{table:list_joint_angle_trajectory}{{3.1}{13}{List of effective joint-angle trajectories with corresponding body joints set for gait angular feature vector \relax }{table.caption.7}{}}
\newlabel{table:list_joint_angle_trajectory@cref}{{[table][1][3]3.1}{[1][12][]13}}
\citation{Wang_04,Araujo_13}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Temporal Displacement}{14}{subsection.3.2.3}}
\citation{Liao_19,Wang_04}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\newlabel{fig:pose_estimation}{{\caption@xref {fig:pose_estimation}{ on input line 121}}{15}{Feature Preprocessing}{figure.caption.8}{}}
\newlabel{fig:pose_estimation@cref}{{[section][3][3]3.3}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Examples of 2D human pose estimation by\nobreakspace  {}\cite  {Cao_19} from RGB images of CASIA dataset (left ones). Detected 25 human body joints with description are shown. (right ones) \relax }}{15}{figure.caption.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Body Part Length Features}{15}{subsection.3.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Fusion of Features}{15}{subsection.3.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feature Preprocessing}{16}{section.3.3}}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Kingma_15}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Data Augmentation}{17}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Network Architecture}{17}{subsection.3.3.2}}
\citation{Kingma_15}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Proposed \gls {rnn} architecture for robust gait recognition. It consists of two \gls {bigru}\nobreakspace  {}\cite  {Schuster_97} layers each of which consists of 80 \gls {gru} cells with one batch normalization and one output softmax layer. The network was fed with 50 dimensional input from 2D pose estimation. Input layer was followed by a batch normalization layer\nobreakspace  {}\cite  {Ioffe_15}. The output of recurrent layers was also batch normalized to standardize the activations and finally fed into an output softmax layer. For the output layer, the number of the output neuron equals to the number of subjects. \relax }}{18}{figure.caption.9}}
\newlabel{fig:rnn_network}{{3.4}{18}{Proposed \gls {rnn} architecture for robust gait recognition. It consists of two \gls {bigru}~\cite {Schuster_97} layers each of which consists of 80 \gls {gru} cells with one batch normalization and one output softmax layer. The network was fed with 50 dimensional input from 2D pose estimation. Input layer was followed by a batch normalization layer~\cite {Ioffe_15}. The output of recurrent layers was also batch normalized to standardize the activations and finally fed into an output softmax layer. For the output layer, the number of the output neuron equals to the number of subjects. \relax }{figure.caption.9}{}}
\newlabel{fig:rnn_network@cref}{{[figure][4][3]3.4}{[1][17][]18}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Training summary of our proposed temporal network. \relax }}{18}{table.caption.10}}
\newlabel{table:summary_tn}{{3.2}{18}{Training summary of our proposed temporal network. \relax }{table.caption.10}{}}
\newlabel{table:summary_tn@cref}{{[table][2][3]3.2}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Training}{18}{subsection.3.3.3}}
\citation{Wen_16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Loss functions}{19}{subsection.3.3.4}}
\newlabel{equ:equ_4}{{3.8}{19}{Loss functions}{equation.3.3.8}{}}
\newlabel{equ:equ_4@cref}{{[equation][8][3]3.8}{[1][19][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Output prediction scheme of our proposed temporal network. Each input clip was considered as a separate video and a sequence of class probabilities was predicted at output. Majority voting scheme was used to process the output to predict the subject ID. \relax }}{20}{figure.caption.11}}
\newlabel{fig:output_prediction}{{3.5}{20}{Output prediction scheme of our proposed temporal network. Each input clip was considered as a separate video and a sequence of class probabilities was predicted at output. Majority voting scheme was used to process the output to predict the subject ID. \relax }{figure.caption.11}{}}
\newlabel{fig:output_prediction@cref}{{[figure][5][3]3.5}{[1][19][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Post-processing}{20}{subsection.3.3.5}}
\newlabel{sec_3.1.8}{{3.3.5}{20}{Post-processing}{subsection.3.3.5}{}}
\newlabel{sec_3.1.8@cref}{{[subsection][5][3,3]3.3.5}{[1][19][]20}}
\newlabel{equ:equ_5}{{3.9}{20}{Post-processing}{equation.3.3.9}{}}
\newlabel{equ:equ_5@cref}{{[equation][9][3]3.9}{[1][20][]20}}
\citation{Redmon_18}
\citation{Tran_15}
\citation{Redmon_18}
\citation{Tran_15}
\citation{Redmon_18}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces  Overview of our proposed multi-view gait recognition network scheme. YOLOv3\nobreakspace  {}\cite  {Redmon_18} was used to detect and locate the walking people in video frames. The input of the network was a clip of 16 consecutive frame which was preprocessed and resized to $112\times 112$ to feed into a 3D convolutional network based on C3D\nobreakspace  {}\cite  {Tran_15}. The network used 3D kernels to exploit spatio-temporal dynamics for viewing angle identification. Thereafter, a temporal network, trained on each viewing angle, was performed subject identification by modeling temporal dynamics from input 2D pose sequence \relax }}{21}{figure.caption.12}}
\newlabel{fig:multi_view_gait_recognition}{{3.6}{21}{Overview of our proposed multi-view gait recognition network scheme. YOLOv3~\cite {Redmon_18} was used to detect and locate the walking people in video frames. The input of the network was a clip of 16 consecutive frame which was preprocessed and resized to $112\times 112$ to feed into a 3D convolutional network based on C3D~\cite {Tran_15}. The network used 3D kernels to exploit spatio-temporal dynamics for viewing angle identification. Thereafter, a temporal network, trained on each viewing angle, was performed subject identification by modeling temporal dynamics from input 2D pose sequence \relax }{figure.caption.12}{}}
\newlabel{fig:multi_view_gait_recognition@cref}{{[figure][6][3]3.6}{[1][21][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Multi-View gait recognition}{21}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Preprocessing}{21}{subsection.3.4.1}}
\citation{Tran_15}
\citation{Karpathy_14}
\citation{Tran_15}
\citation{Tran_15}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces  Proposed 3D-CNN for video angle identification. Last 3 layers of a pretrained C3D\nobreakspace  {}\cite  {Tran_15} network has been replaced by a fully connected layer of 128 neurons followed a final softmax layer of 11 neurons to classify 11 different walking direction in CASIA-B dataset. \relax }}{22}{figure.caption.13}}
\newlabel{fig:3D_CNN}{{3.7}{22}{Proposed 3D-CNN for video angle identification. Last 3 layers of a pretrained C3D~\cite {Tran_15} network has been replaced by a fully connected layer of 128 neurons followed a final softmax layer of 11 neurons to classify 11 different walking direction in CASIA-B dataset. \relax }{figure.caption.13}{}}
\newlabel{fig:3D_CNN@cref}{{[figure][7][3]3.7}{[1][22][]22}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Training summary of our proposed 3D-CNN network. \relax }}{22}{table.caption.14}}
\newlabel{table:summary_3dcnn}{{3.3}{22}{Training summary of our proposed 3D-CNN network. \relax }{table.caption.14}{}}
\newlabel{table:summary_3dcnn@cref}{{[table][3][3]3.3}{[1][22][]22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Network architecture}{22}{subsection.3.4.2}}
\citation{Yu_06}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Training}{23}{subsection.3.4.3}}
\citation{Yu_06}
\citation{Hofmann_14}
\citation{Noriko_18}
\citation{Sarkar_05}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results \& Discussion}{24}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{result_discussion}{{4}{24}{Results \& Discussion}{chapter.4}{}}
\newlabel{result_discussion@cref}{{[chapter][4][]4}{[1][24][]24}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset}{24}{section.4.1}}
\citation{Wang_03}
\citation{Goffredo_08}
\citation{Liu_16}
\citation{Lima_19}
\citation{Kusakunniran_09}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  Sample video frames of CASIA A and CASIA B dataset. In top, some of the sample images from CASIA A dataset are shown where subjects walking along straight line in 3 different view angle, and in bottom, CASIA B dataset is shown with its 11 viewing angle. \relax }}{25}{figure.caption.15}}
\newlabel{fig:casia_dataset}{{4.1}{25}{Sample video frames of CASIA A and CASIA B dataset. In top, some of the sample images from CASIA A dataset are shown where subjects walking along straight line in 3 different view angle, and in bottom, CASIA B dataset is shown with its 11 viewing angle. \relax }{figure.caption.15}{}}
\newlabel{fig:casia_dataset@cref}{{[figure][1][4]4.1}{[1][25][]25}}
\citation{Wang_03}
\citation{Goffredo_08}
\citation{Liu_16}
\citation{Lima_19}
\citation{Kusakunniran_09}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison among different state-of-the-art gait recognition methods without view variation on all three view angles of CASIA-A dataset. It has been seen that, proposed method achieves best performance in correct class recognition rate on view angle ${45^{\circ }}$. Overall, it gets higher average recognition rates \textbf  {98.3\%} and outperforms other state-of-the-art methods by a large margin. \relax }}{26}{table.caption.16}}
\newlabel{table:casia_a_result}{{4.1}{26}{Comparison among different state-of-the-art gait recognition methods without view variation on all three view angles of CASIA-A dataset. It has been seen that, proposed method achieves best performance in correct class recognition rate on view angle ${45^{\circ }}$. Overall, it gets higher average recognition rates \textbf {98.3\%} and outperforms other state-of-the-art methods by a large margin. \relax }{table.caption.16}{}}
\newlabel{table:casia_a_result@cref}{{[table][1][4]4.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Single-view Gait Recognition}{26}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Experimental Evaluation on CASIA-A dataset}{26}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Experimental Evaluation on CASIA-B Dataset}{26}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Setup}{26}{section*.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  Comparison in correct class recognition rate (CCR) at different viewing angles among proposed method with other prevailing gait recognition methods proposed in literature on CASIA A dataset. Our method achieves $ 100\% $ class recognition rate on all of the view angles which proved the efficacy of the proposed method. \relax }}{27}{figure.caption.17}}
\newlabel{fig:casia_a_result}{{4.2}{27}{Comparison in correct class recognition rate (CCR) at different viewing angles among proposed method with other prevailing gait recognition methods proposed in literature on CASIA A dataset. Our method achieves $ 100\% $ class recognition rate on all of the view angles which proved the efficacy of the proposed method. \relax }{figure.caption.17}{}}
\newlabel{fig:casia_a_result@cref}{{[figure][2][4]4.2}{[1][26][]27}}
\citation{Yu_19}
\citation{Liao_17}
\citation{Liao_19}
\citation{Yu_17_spae}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Experimental setup for the CASIA B dataset. The dataset is divided into two different setups to organize two different types of experiment. the evaluation is subdivided into a gallery set and a probe set. Gallery set consists of the first 4 normal walking sequences of each subject and the probe set contains rest of the walking sequences \relax }}{28}{table.caption.19}}
\newlabel{table:caisab_setup}{{4.2}{28}{Experimental setup for the CASIA B dataset. The dataset is divided into two different setups to organize two different types of experiment. the evaluation is subdivided into a gallery set and a probe set. Gallery set consists of the first 4 normal walking sequences of each subject and the probe set contains rest of the walking sequences \relax }{table.caption.19}{}}
\newlabel{table:caisab_setup@cref}{{[table][2][4]4.2}{[1][27][]28}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Correct class recognition rate (\gls {ccr}) of proposed method in all three probe sets of CASIA B dataset. Here, column represents a specific view of gallery and probe set. It has been observed that the probe set of normal walking (\textit  {ProbeNM}) achieves $ \textbf  {99.41\%}$ average recognition rate while the ProbeBG and ProbeCL set achieve $ \textbf  {97.80\%}$ and $\textbf  {82.82\%}$ average recognition rates respectively. \relax }}{28}{table.caption.21}}
\newlabel{table:resutl_without_view}{{4.3}{28}{Correct class recognition rate (\gls {ccr}) of proposed method in all three probe sets of CASIA B dataset. Here, column represents a specific view of gallery and probe set. It has been observed that the probe set of normal walking (\textit {ProbeNM}) achieves $ \textbf {99.41\%}$ average recognition rate while the ProbeBG and ProbeCL set achieve $ \textbf {97.80\%}$ and $\textbf {82.82\%}$ average recognition rates respectively. \relax }{table.caption.21}{}}
\newlabel{table:resutl_without_view@cref}{{[table][3][4]4.3}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsubsection}{Results on Single-View Gait Recognition of CASIA B Dataset without View Variation}{28}{section*.20}}
\citation{Liao_19}
\citation{Yu_17_spae}
\citation{Yu_19}
\citation{Liao_19}
\citation{Yu_19}
\citation{Liao_19}
\citation{Yu_17_spae}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Comparison between the proposed method and other state-of-the-art gait recognition methods in CASIA B dataset without view variation. It has been observed that the proposed method outperforms other methods in all three probe set of CASIA B dataset. As the proposed method doesn't depend on any body point higher than knee, it shows the robustness towards these covariate factors. It also achieves higher average correct class recognition rate (CCR) $\textbf  {93.34\%}$ by outperforming other methods at a significant margin. \relax }}{29}{table.caption.23}}
\newlabel{table:comp_casia_b_without_view}{{4.4}{29}{Comparison between the proposed method and other state-of-the-art gait recognition methods in CASIA B dataset without view variation. It has been observed that the proposed method outperforms other methods in all three probe set of CASIA B dataset. As the proposed method doesn't depend on any body point higher than knee, it shows the robustness towards these covariate factors. It also achieves higher average correct class recognition rate (CCR) $\textbf {93.34\%}$ by outperforming other methods at a significant margin. \relax }{table.caption.23}{}}
\newlabel{table:comp_casia_b_without_view@cref}{{[table][4][4]4.4}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison on Single-View Gait Recognition of CASIA B Dataset with State-of-the-art Methods without View Variation}{29}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{Results on Single-View Gait Recognition of CASIA B Dataset with View Variation}{29}{section*.25}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison on Single-View Gait Recognition of CASIA B Dataset with State-of-the-art Methods with View Variation}{29}{section*.27}}
\citation{Yu_17_spae}
\citation{Yu_19}
\citation{Liao_19}
\citation{Wu_17}
\citation{Kusakunniran_14}
\citation{Kusakunniran_10}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Correct class recognition rates (\%) of the proposed method with other state-of-the-art methods on all three probe set of CASIA-B dataset without view variation. Proposed method demonstrates better performance compared to other by achieving $89.64\%$ and $96.45\%$ in two covariate conditions of CASIA-B dataset \textit  {ProbeCL}, and \textit  {ProbeBG} respectively. The result proves the robustness of proposed pose-based temporal network against carrying and clothing conditions variations. \relax }}{30}{figure.caption.24}}
\newlabel{fig:comp_casia_b_without_view}{{4.3}{30}{Correct class recognition rates (\%) of the proposed method with other state-of-the-art methods on all three probe set of CASIA-B dataset without view variation. Proposed method demonstrates better performance compared to other by achieving $89.64\%$ and $96.45\%$ in two covariate conditions of CASIA-B dataset \textit {ProbeCL}, and \textit {ProbeBG} respectively. The result proves the robustness of proposed pose-based temporal network against carrying and clothing conditions variations. \relax }{figure.caption.24}{}}
\newlabel{fig:comp_casia_b_without_view@cref}{{[figure][3][4]4.3}{[1][29][]30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Cross-View Gait Recognition}{30}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Comparison with the State-of-the-art Methods of CASIA B Dataset on Cross-View Gait Recognition}{30}{subsection.4.3.1}}
\citation{table:comp_casia_b_cross_view}
\citation{Wu_17}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The average recognition rates for all three probe sets of CASIA B dataset. Each row represents the average value of all eleven probe angles at a specific gallery angle ($ \theta _g $) in all three probe sets. \relax }}{31}{table.caption.26}}
\newlabel{table:comp_casia_b_with_view}{{4.5}{31}{The average recognition rates for all three probe sets of CASIA B dataset. Each row represents the average value of all eleven probe angles at a specific gallery angle ($ \theta _g $) in all three probe sets. \relax }{table.caption.26}{}}
\newlabel{table:comp_casia_b_with_view@cref}{{[table][5][4]4.5}{[1][29][]31}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Comparison among different state-of-the-art methods for gait recognition with view variation in all three probe sets of CASIA B dataset. Each row represents the average value of all the gallery view's average recognition rate. It has been seen that, similar to first experiment, the proposed method achieves higher performance in two different probe set (\textit  {ProbeBG}, \textit  {ProbeCL}) and comparable performance in normal walking with to other prevailing methods. \relax }}{31}{table.caption.28}}
\newlabel{table:comp_casia_b_with_view}{{4.6}{31}{Comparison among different state-of-the-art methods for gait recognition with view variation in all three probe sets of CASIA B dataset. Each row represents the average value of all the gallery view's average recognition rate. It has been seen that, similar to first experiment, the proposed method achieves higher performance in two different probe set (\textit {ProbeBG}, \textit {ProbeCL}) and comparable performance in normal walking with to other prevailing methods. \relax }{table.caption.28}{}}
\newlabel{table:comp_casia_b_with_view@cref}{{[table][6][4]4.6}{[1][30][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  Comparison with different state-of-the-art methods for gait recognition with view variation in all three probe set of CASIA B dataset. Here, the value reported for each algorithm is the average of all the gallery view's average \gls {ccr}. Proposed method outperforms other state-of-the-art methods achieving $47.23\%$ and $33.46\%$ in two covariate conditions \textit  {ProbeBG}, and \textit  {ProbeCL} respectively. \relax }}{32}{figure.caption.29}}
\newlabel{fig:comp_casia_b_with_view}{{4.4}{32}{Comparison with different state-of-the-art methods for gait recognition with view variation in all three probe set of CASIA B dataset. Here, the value reported for each algorithm is the average of all the gallery view's average \gls {ccr}. Proposed method outperforms other state-of-the-art methods achieving $47.23\%$ and $33.46\%$ in two covariate conditions \textit {ProbeBG}, and \textit {ProbeCL} respectively. \relax }{figure.caption.29}{}}
\newlabel{fig:comp_casia_b_with_view@cref}{{[figure][4][4]4.4}{[1][30][]32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Comparison of our proposed method with the previous best results of cross-view gait recognition at different probe angles of CASIA B dataset by \gls {ccr}(\%). The network was trained according to experimental setup B to have the same setup with other methods. \relax }}{32}{table.caption.30}}
\newlabel{table:comp_casia_b_cross_view}{{4.7}{32}{Comparison of our proposed method with the previous best results of cross-view gait recognition at different probe angles of CASIA B dataset by \gls {ccr}(\%). The network was trained according to experimental setup B to have the same setup with other methods. \relax }{table.caption.30}{}}
\newlabel{table:comp_casia_b_cross_view@cref}{{[table][7][4]4.7}{[1][31][]32}}
\citation{Dupuis_13}
\citation{Isaac_17}
\citation{Choudhury_15}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Comparison with other state-of-the-art methods on all three probe set of CASIA-B dataset in multi-view gait recognition. From the comparison, it is been observed that proposed two-stage network achieves higher average recognition rates in 8 of 11 different probe angles.\relax }}{33}{table.caption.31}}
\newlabel{table:comp_multi_view}{{4.8}{33}{Comparison with other state-of-the-art methods on all three probe set of CASIA-B dataset in multi-view gait recognition. From the comparison, it is been observed that proposed two-stage network achieves higher average recognition rates in 8 of 11 different probe angles.\relax }{table.caption.31}{}}
\newlabel{table:comp_multi_view@cref}{{[table][8][4]4.8}{[1][33][]33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Multi-View Gait Recognition}{33}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Comparison with the state-of-the-art methods on multi-view gait recognition}{33}{subsection.4.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Correct walking direction identification rate (\%) of proposed 3D-CNN network on all three probe set of CASIA-B dataset. The network achieved \textbf  {100\%} identification accuracy in all of the 11 view angles. \relax }}{34}{table.caption.32}}
\newlabel{table:result_wd_identification}{{4.9}{34}{Correct walking direction identification rate (\%) of proposed 3D-CNN network on all three probe set of CASIA-B dataset. The network achieved \textbf {100\%} identification accuracy in all of the 11 view angles. \relax }{table.caption.32}{}}
\newlabel{table:result_wd_identification@cref}{{[table][9][4]4.9}{[1][33][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  Average recognition rates(\%) of the proposed method compared to the other state-of-the-art methods in multi-view gait recognition. Proposed method achieves higher average recognition accuracy on 8 of total 11 probe angles of CASIA-B dataset compared other methods in literature. \relax }}{34}{figure.caption.33}}
\newlabel{fig:comp_casia_b_multi_view}{{4.5}{34}{Average recognition rates(\%) of the proposed method compared to the other state-of-the-art methods in multi-view gait recognition. Proposed method achieves higher average recognition accuracy on 8 of total 11 probe angles of CASIA-B dataset compared other methods in literature. \relax }{figure.caption.33}{}}
\newlabel{fig:comp_casia_b_multi_view@cref}{{[figure][5][4]4.5}{[1][34][]34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{35}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusion}{{5}{35}{Conclusion}{chapter.5}{}}
\newlabel{conclusion@cref}{{[chapter][5][]5}{[1][35][]35}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Summary of Our Work}{35}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Future Prospects of Our Work}{35}{section.5.2}}
\bibstyle{IEEEtran}
\bibdata{buet_msc_thesis}
\bibcite{Cao_19}{1}
\bibcite{Schuster_97}{2}
\bibcite{Ioffe_15}{3}
\bibcite{Redmon_18}{4}
\bibcite{Tran_15}{5}
\bibcite{Yu_06}{6}
\bibcite{Rida_19}{7}
\bibcite{Han_06}{8}
\bibcite{Bashir_09}{9}
\bibcite{Lam_11}{10}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Bibliography}}{36}{section.5.2}}
\bibcite{Huang_12}{11}
\bibcite{Chao_19}{12}
\bibcite{Yam_04}{13}
\bibcite{Ariyanto_11}{14}
\bibcite{Tafazzoli_10}{15}
\bibcite{Feng_16}{16}
\bibcite{Wu_17}{17}
\bibcite{Shiraga_16}{18}
\bibcite{Wolf_16}{19}
\bibcite{Zhang_16}{20}
\bibcite{Yu_17}{21}
\bibcite{Yu_19}{22}
\bibcite{Liao_17}{23}
\bibcite{Liao_19}{24}
\bibcite{Song_17}{25}
\bibcite{Du_15}{26}
\bibcite{Cunado_97}{27}
\bibcite{Wang_04}{28}
\bibcite{Araujo_13}{29}
\bibcite{Kingma_15}{30}
\bibcite{Wen_16}{31}
\bibcite{Karpathy_14}{32}
\bibcite{Hofmann_14}{33}
\bibcite{Noriko_18}{34}
\bibcite{Sarkar_05}{35}
\bibcite{Wang_03}{36}
\bibcite{Goffredo_08}{37}
\bibcite{Liu_16}{38}
\bibcite{Lima_19}{39}
\bibcite{Kusakunniran_09}{40}
\bibcite{Yu_17_spae}{41}
\bibcite{Kusakunniran_14}{42}
\bibcite{Kusakunniran_10}{43}
\bibcite{Dupuis_13}{44}
\bibcite{Isaac_17}{45}
\bibcite{Choudhury_15}{46}
