\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{gait_thesis.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Certification}}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Candidate's Declaration}}{ii}{DTLrowi.1.5}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Abstract}}{iii}{DTLrowi.2.1}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Acknowledgement}}{iv}{DTLrowi.2.1}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{section*.2}}
\@writefile{toc}{\contentsline {chapter}{\textbf  {List of Abbreviations}}{xi}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\newlabel{intro@cref}{{[chapter][1][]1}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Person re-identification}{1}{section.1.1}}
\citation{Boyd_05}
\citation{Benabdelkader_02}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A basic topology of person re-identification in a multi-camera network environment}}{2}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:person_reid}{{1.1}{2}{A basic topology of person re-identification in a multi-camera network environment}{figure.caption.5}{}}
\newlabel{fig:person_reid@cref}{{[figure][1][1]1.1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Gait Recognition}{2}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Definition}{2}{subsection.1.2.1}}
\citation{Michele_17}
\citation{Juen_14}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A basic topology of person re-identification in a multi-camera network environment}}{3}{figure.caption.6}}
\newlabel{fig:gait_challenges}{{1.2}{3}{A basic topology of person re-identification in a multi-camera network environment}{figure.caption.6}{}}
\newlabel{fig:gait_challenges@cref}{{[figure][2][1]1.2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Advantages}{3}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Challenges}{4}{subsection.1.2.3}}
\citation{Lee_14}
\citation{karpathy_14}
\citation{Cao_19}
\citation{Song_17,Du_15}
\citation{Mao_15}
\citation{Song_17,Du_15}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Problem Definition}{5}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Objectives of the Thesis}{5}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Overview of the Thesis}{6}{section.1.5}}
\citation{Yu_06}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Contributions}{7}{section.1.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Thesis Outline}{8}{section.1.7}}
\citation{Rida_19}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:literature_review}{{2}{9}{Literature Review}{chapter.2}{}}
\newlabel{ch:literature_review@cref}{{[chapter][2][]2}{[1][9][]9}}
\citation{Benabdelkader_02,Liu_04,Han_06,Bashir_09,Lam_11}
\citation{Benabdelkader_02}
\citation{Liu_04}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Appearance-based Methods}{10}{section.2.1}}
\newlabel{sec:appearance_based_methods}{{2.1}{10}{Appearance-based Methods}{section.2.1}{}}
\newlabel{sec:appearance_based_methods@cref}{{[section][1][2]2.1}{[1][10][]10}}
\citation{Han_06}
\citation{Bashir_09}
\citation{Lam_11}
\citation{Wang_12}
\citation{Huang_12}
\citation{Wang_12}
\citation{Sarkar_05}
\citation{Yam_04,Gu_10,Wagg_04}
\citation{Pavlovic_99}
\citation{Isard_98}
\citation{Wagg_04}
\citation{Shutler_04}
\citation{Yam_04}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Model-based Methods}{12}{section.2.2}}
\newlabel{sec:model_based_methods}{{2.2}{12}{Model-based Methods}{section.2.2}{}}
\newlabel{sec:model_based_methods@cref}{{[section][2][2]2.2}{[1][11][]12}}
\citation{Gu_10}
\citation{Wu_17,Shiraga_16,Wolf_16,Zhang_16,Yu_17,Yu_19}
\citation{Wu_17}
\citation{Shiraga_16}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning for Gait Recognition}{13}{section.2.3}}
\newlabel{sec:deep_learning_gait_rec}{{2.3}{13}{Deep Learning for Gait Recognition}{section.2.3}{}}
\newlabel{sec:deep_learning_gait_rec@cref}{{[section][3][2]2.3}{[1][13][]13}}
\citation{Wolf_16}
\citation{Gross_01}
\citation{Sarkar_05}
\citation{Yu_06}
\citation{Zhang_16}
\citation{Yu_17}
\citation{Yu_19}
\citation{Wei_16,Cao_19}
\citation{Wei_16}
\citation{Cao_19}
\citation{Cai_16}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Pose Estimation}{14}{section.2.4}}
\newlabel{sec:pose_estimation}{{2.4}{14}{Pose Estimation}{section.2.4}{}}
\newlabel{sec:pose_estimation@cref}{{[section][4][2]2.4}{[1][14][]14}}
\citation{Feng_16,Liao_17,Liao_19}
\citation{Feng_16}
\citation{Liao_17}
\citation{Liao_19}
\citation{Song_17,Du_15}
\citation{Song_17}
\citation{Du_15}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Pose-based Gait Recognition}{15}{section.2.5}}
\newlabel{sec:pose_based_gait_rec}{{2.5}{15}{Pose-based Gait Recognition}{section.2.5}{}}
\newlabel{sec:pose_based_gait_rec@cref}{{[section][5][2]2.5}{[1][15][]15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{16}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:methodology}{{3}{16}{Methodology}{chapter.3}{}}
\newlabel{ch:methodology@cref}{{[chapter][3][]3}{[1][16][]16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Deep Learning Basics}{16}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Deep learning}{16}{subsection.3.1.1}}
\citation{Krizhevsky_12}
\citation{Mao_15}
\citation{kal_13}
\citation{karpathy_14}
\citation{graves_13}
\citation{Krizhevsky_12}
\citation{karpathy_14}
\citation{Tran_15}
\citation{Hubel_68}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Computing output activation of a convolutional layer}}{17}{figure.caption.7}}
\newlabel{fig:convolution}{{3.1}{17}{Computing output activation of a convolutional layer}{figure.caption.7}{}}
\newlabel{fig:convolution@cref}{{[figure][1][3]3.1}{[1][17][]17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Convolutional Neural Networks}{17}{subsection.3.1.2}}
\citation{colah_15}
\citation{colah_15}
\citation{mikolov_12}
\citation{graves_13}
\citation{kal_13}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A typical rolled representation of a recurrent neural network}}{18}{figure.caption.8}}
\newlabel{fig:RNN_unrolled}{{3.2}{18}{A typical rolled representation of a recurrent neural network}{figure.caption.8}{}}
\newlabel{fig:RNN_unrolled@cref}{{[figure][2][3]3.2}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The computational graph of a unrolled recurrent network that maps an input sequence of $ x $ values to a corresponding sequence of output $ o $ values \relax }}{19}{figure.caption.9}}
\newlabel{fig:RNN_unrolling}{{3.3}{19}{The computational graph of a unrolled recurrent network that maps an input sequence of $ x $ values to a corresponding sequence of output $ o $ values \relax }{figure.caption.9}{}}
\newlabel{fig:RNN_unrolling@cref}{{[figure][3][3]3.3}{[1][19][]19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Recurrent Neural Networks}{19}{subsection.3.1.3}}
\citation{Bengio_94}
\citation{Hochreiter_01}
\citation{colah_15}
\citation{colah_15}
\newlabel{rnn_unroll}{{3.2}{20}{Recurrent Neural Networks}{equation.3.1.2}{}}
\newlabel{rnn_unroll@cref}{{[equation][2][3]3.2}{[1][19][]20}}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\citation{colah_15}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A Long Short Term Memory (LSTM)}}{21}{figure.caption.10}}
\newlabel{fig:LSTM}{{3.4}{21}{A Long Short Term Memory (LSTM)}{figure.caption.10}{}}
\newlabel{fig:LSTM@cref}{{[figure][4][3]3.4}{[1][20][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The internal state of LSTMs}}{21}{figure.caption.11}}
\newlabel{fig:LSTM_state}{{3.5}{21}{The internal state of LSTMs}{figure.caption.11}{}}
\newlabel{fig:LSTM_state@cref}{{[figure][5][3]3.5}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Long Short-Term Memory}{21}{subsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The LSTM forget gate}}{22}{figure.caption.12}}
\newlabel{fig:LSTM_forget_gate}{{3.6}{22}{The LSTM forget gate}{figure.caption.12}{}}
\newlabel{fig:LSTM_forget_gate@cref}{{[figure][6][3]3.6}{[1][21][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The LSTM input gate}}{22}{figure.caption.13}}
\newlabel{fig:LSTM_input_gate}{{3.7}{22}{The LSTM input gate}{figure.caption.13}{}}
\newlabel{fig:LSTM_input_gate@cref}{{[figure][7][3]3.7}{[1][21][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The LSTM output gate}}{22}{figure.caption.14}}
\newlabel{fig:LSTM_output_gate}{{3.8}{22}{The LSTM output gate}{figure.caption.14}{}}
\newlabel{fig:LSTM_output_gate@cref}{{[figure][8][3]3.8}{[1][21][]22}}
\citation{colah_15}
\citation{colah_15}
\citation{Cho_14}
\newlabel{eq:LSTM_forget_gate}{{3.3}{23}{Long Short-Term Memory}{equation.3.1.3}{}}
\newlabel{eq:LSTM_forget_gate@cref}{{[equation][3][3]3.3}{[1][23][]23}}
\newlabel{eq:LSTM_input_gate}{{3.4}{23}{Long Short-Term Memory}{equation.3.1.4}{}}
\newlabel{eq:LSTM_input_gate@cref}{{[equation][4][3]3.4}{[1][23][]23}}
\newlabel{eq:LSTM_state_update}{{3.5}{23}{Long Short-Term Memory}{equation.3.1.5}{}}
\newlabel{eq:LSTM_state_update@cref}{{[equation][5][3]3.5}{[1][23][]23}}
\newlabel{eq:LSTM_output_gate}{{3.6}{23}{Long Short-Term Memory}{equation.3.1.6}{}}
\newlabel{eq:LSTM_output_gate@cref}{{[equation][6][3]3.6}{[1][23][]23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Gated Recurrent Units (GRUs)}}{24}{figure.caption.15}}
\newlabel{fig:GRU}{{3.9}{24}{Gated Recurrent Units (GRUs)}{figure.caption.15}{}}
\newlabel{fig:GRU@cref}{{[figure][9][3]3.9}{[1][23][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Gated Recurrent Unit}{24}{subsection.3.1.5}}
\newlabel{sec:GRU}{{3.1.5}{24}{Gated Recurrent Unit}{subsection.3.1.5}{}}
\newlabel{sec:GRU@cref}{{[subsection][5][3,1]3.1.5}{[1][23][]24}}
\citation{Chung_14}
\citation{Schuster_97}
\citation{Graves_08}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The architecture of a vanilla bidirectional recurrent neural network \relax }}{25}{figure.caption.16}}
\newlabel{fig:bidirectional_rnn}{{3.10}{25}{The architecture of a vanilla bidirectional recurrent neural network \relax }{figure.caption.16}{}}
\newlabel{fig:bidirectional_rnn@cref}{{[figure][10][3]3.10}{[1][25][]25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The architecture of a bidirectional gated recurrent neural network \relax }}{26}{figure.caption.17}}
\newlabel{fig:bidirectional_gru}{{3.11}{26}{The architecture of a bidirectional gated recurrent neural network \relax }{figure.caption.17}{}}
\newlabel{fig:bidirectional_gru@cref}{{[figure][11][3]3.11}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Bidirectional RNNs}{26}{subsection.3.1.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6.1}Bidirectional GRU}{26}{subsubsection.3.1.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}Regularization for Deep Learning}{27}{subsection.3.1.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.1}L2 Regularization}{27}{subsubsection.3.1.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.2}Dropout}{27}{subsubsection.3.1.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.3}Dataset Augmentation}{27}{subsubsection.3.1.7.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.4}Early Stopping of Training}{27}{subsubsection.3.1.7.4}}
\citation{Song_17}
\citation{Marchand_16}
\citation{Ke_10}
\citation{Liao_19}
\citation{Cao_19}
\citation{Cao_19}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.5}Noise Robustness}{28}{subsubsection.3.1.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Human Pose Estimation}{28}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Types of Pose Estimation}{28}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Realtime multi-person 2D pose estimation using OpenPose algorithm}}{29}{figure.caption.18}}
\newlabel{fig:openpose_demo}{{3.12}{29}{Realtime multi-person 2D pose estimation using OpenPose algorithm}{figure.caption.18}{}}
\newlabel{fig:openpose_demo@cref}{{[figure][12][3]3.12}{[1][28][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Techniques for Pose Estimation}{29}{subsection.3.2.2}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces An example of a bottom up approach}}{30}{figure.caption.19}}
\newlabel{fig:openpose_bottom_up}{{3.13}{30}{An example of a bottom up approach}{figure.caption.19}{}}
\newlabel{fig:openpose_bottom_up@cref}{{[figure][13][3]3.13}{[1][30][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Network architecture of the multi-stage CNN}}{30}{figure.caption.20}}
\newlabel{fig:openpose_architecture}{{3.14}{30}{Network architecture of the multi-stage CNN}{figure.caption.20}{}}
\newlabel{fig:openpose_architecture@cref}{{[figure][14][3]3.14}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Introduction to OpenPose Library}{30}{subsection.3.2.3}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cunado_97}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Extracting Spatio-Temporal Feature Vector}{31}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}2D Body Joint Features}{31}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The overview of the proposed framework for gait recognition}}{32}{figure.caption.21}}
\newlabel{fig:overview_proposed_method}{{3.15}{32}{The overview of the proposed framework for gait recognition}{figure.caption.21}{}}
\newlabel{fig:overview_proposed_method@cref}{{[figure][15][3]3.15}{[1][31][]32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Scheme for the four different types of feature extraction process of the proposed method}}{33}{figure.caption.22}}
\newlabel{fig:extracted_features}{{3.16}{33}{Scheme for the four different types of feature extraction process of the proposed method}{figure.caption.22}{}}
\newlabel{fig:extracted_features@cref}{{[figure][16][3]3.16}{[1][31][]33}}
\citation{Wang_04}
\newlabel{equ:normalization_raw_joint}{{3.13}{34}{2D Body Joint Features}{equation.3.3.13}{}}
\newlabel{equ:normalization_raw_joint@cref}{{[equation][13][3]3.13}{[1][34][]34}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces List of selected joint-angle trajectories with corresponding body joints set in order to form a joint angular feature vector. \relax }}{35}{table.caption.23}}
\newlabel{table:list_joint_angle_trajectory}{{3.1}{35}{List of selected joint-angle trajectories with corresponding body joints set in order to form a joint angular feature vector. \relax }{table.caption.23}{}}
\newlabel{table:list_joint_angle_trajectory@cref}{{[table][1][3]3.1}{[1][35][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Joint Angular Trajectory}{35}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Temporal Displacement}{35}{subsection.3.3.3}}
\citation{Wang_04,Araujo_13}
\citation{Liao_19,Wang_04}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Body Part Length Features}{36}{subsection.3.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Fusion of Features}{36}{subsection.3.3.5}}
\citation{Cao_19}
\citation{Cao_19}
\citation{Cao_19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Examples of 2D human pose estimation from RGB images of CASIA dataset}}{37}{figure.caption.24}}
\newlabel{fig:pose_estimation}{{3.17}{37}{Examples of 2D human pose estimation from RGB images of CASIA dataset}{figure.caption.24}{}}
\newlabel{fig:pose_estimation@cref}{{[figure][17][3]3.17}{[1][37][]37}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Feature Preprocessing}{37}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Handling Missing Joint Information}{37}{subsection.3.4.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm for Handling Missing Joints Information\relax }}{38}{algorithm.1}}
\newlabel{alg:missing_data}{{1}{38}{Algorithm for Handling Missing Joints Information\relax }{algorithm.1}{}}
\newlabel{alg:missing_data@cref}{{[algorithm][1][]1}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Forming Feature Map}{38}{subsection.3.4.2}}
\newlabel{equ:feature_preprocess}{{3.17}{38}{Forming Feature Map}{equation.3.4.17}{}}
\newlabel{equ:feature_preprocess@cref}{{[equation][17][3]3.17}{[1][38][]38}}
\citation{Yu_06}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Data Augmentation}{39}{subsection.3.4.3}}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Schuster_97}
\citation{Ioffe_15}
\citation{Graves_05}
\citation{Schuster_97}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Proposed network architecture for robust gait recognition}}{40}{figure.caption.25}}
\newlabel{fig:rnn_network}{{3.18}{40}{Proposed network architecture for robust gait recognition}{figure.caption.25}{}}
\newlabel{fig:rnn_network@cref}{{[figure][18][3]3.18}{[1][40][]40}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Single-View Gait Recognition}{40}{section.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Network Architecture}{40}{subsection.3.5.1}}
\citation{Ioffe_15}
\citation{Wen_16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Loss Function}{41}{subsection.3.5.2}}
\newlabel{equ:loss_functions}{{3.19}{41}{Loss Function}{equation.3.5.19}{}}
\newlabel{equ:loss_functions@cref}{{[equation][19][3]3.19}{[1][41][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Output prediction scheme of our proposed network}}{42}{figure.caption.26}}
\newlabel{fig:output_prediction}{{3.19}{42}{Output prediction scheme of our proposed network}{figure.caption.26}{}}
\newlabel{fig:output_prediction@cref}{{[figure][19][3]3.19}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Post-processing}{42}{subsection.3.5.3}}
\newlabel{subsec_post_process}{{3.5.3}{42}{Post-processing}{subsection.3.5.3}{}}
\newlabel{subsec_post_process@cref}{{[subsection][3][3,5]3.5.3}{[1][41][]42}}
\newlabel{equ:timestep_sequence}{{3.20}{42}{Post-processing}{equation.3.5.20}{}}
\newlabel{equ:timestep_sequence@cref}{{[equation][20][3]3.20}{[1][42][]42}}
\citation{Kingma_15}
\citation{Kingma_15}
\citation{keras}
\citation{opencv}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Training summary of our proposed temporal network. \relax }}{43}{table.caption.27}}
\newlabel{table:summary_tn}{{3.2}{43}{Training summary of our proposed temporal network. \relax }{table.caption.27}{}}
\newlabel{table:summary_tn@cref}{{[table][2][3]3.2}{[1][43][]43}}
\newlabel{equ:predicted_class}{{3.21}{43}{Post-processing}{equation.3.5.21}{}}
\newlabel{equ:predicted_class@cref}{{[equation][21][3]3.21}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Training and Implementation Details}{43}{subsection.3.5.4}}
\citation{Redmon_18}
\citation{Tran_15}
\citation{Redmon_18}
\citation{Tran_15}
\citation{Redmon_18}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Overview of our proposed view angle identification network scheme}}{44}{figure.caption.28}}
\newlabel{fig:view_angle_identification}{{3.20}{44}{Overview of our proposed view angle identification network scheme}{figure.caption.28}{}}
\newlabel{fig:view_angle_identification@cref}{{[figure][20][3]3.20}{[1][44][]44}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Multi-View Gait Recognition}{44}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Preprocessing}{44}{subsection.3.6.1}}
\citation{Tran_15}
\citation{karpathy_14}
\citation{Tran_15}
\citation{Tran_15}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Training summary of our proposed 3D-CNN network. \relax }}{45}{table.caption.29}}
\newlabel{table:summary_3dcnn}{{3.3}{45}{Training summary of our proposed 3D-CNN network. \relax }{table.caption.29}{}}
\newlabel{table:summary_3dcnn@cref}{{[table][3][3]3.3}{[1][45][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Proposed 3D-CNN for video angle identification}}{45}{figure.caption.30}}
\newlabel{fig:3D_CNN}{{3.21}{45}{Proposed 3D-CNN for video angle identification}{figure.caption.30}{}}
\newlabel{fig:3D_CNN@cref}{{[figure][21][3]3.21}{[1][45][]45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}3D Convolution for Video Classification}{45}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Network for View Angle Identification}{45}{subsection.3.6.3}}
\citation{Yu_06}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Poposed two-stage network for multi-view gait recognition.}}{46}{figure.caption.31}}
\newlabel{fig:two-stage_network}{{3.22}{46}{Poposed two-stage network for multi-view gait recognition}{figure.caption.31}{}}
\newlabel{fig:two-stage_network@cref}{{[figure][22][3]3.22}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Two-Stage Network for Multi-View Gait Recognition}{46}{subsection.3.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}Training Details}{46}{subsection.3.6.5}}
\citation{Yu_06}
\citation{Hofmann_14}
\citation{Shutler_04}
\citation{Noriko_18}
\citation{Sarkar_05}
\citation{Sarkar_05}
\citation{Hofmann_14}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimental Results}{47}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experimental_result}{{4}{47}{Experimental Results}{chapter.4}{}}
\newlabel{ch:experimental_result@cref}{{[chapter][4][]4}{[1][47][]47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset}{47}{section.4.1}}
\newlabel{sec:datasets}{{4.1}{47}{Dataset}{section.4.1}{}}
\newlabel{sec:datasets@cref}{{[section][1][4]4.1}{[1][47][]47}}
\citation{Gross_01}
\citation{Shutler_04}
\citation{Noriko_18}
\citation{Yu_06}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Sample video frames of CASIA A and CASIA B dataset}}{48}{figure.caption.32}}
\newlabel{fig:casia_dataset}{{4.1}{48}{Sample video frames of CASIA A and CASIA B dataset}{figure.caption.32}{}}
\newlabel{fig:casia_dataset@cref}{{[figure][1][4]4.1}{[1][47][]48}}
\citation{Wang_03}
\citation{Goffredo_08}
\citation{Liu_16}
\citation{Lima_19}
\citation{Kusakunniran_09}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison in CCR among proposed method with other prevailing gait recognition methods at different view angles on CASIA A dataset}}{49}{figure.caption.33}}
\newlabel{fig:casia_a_result}{{4.2}{49}{Comparison in CCR among proposed method with other prevailing gait recognition methods at different view angles on CASIA A dataset}{figure.caption.33}{}}
\newlabel{fig:casia_a_result@cref}{{[figure][2][4]4.2}{[1][49][]49}}
\citation{Wang_03}
\citation{Goffredo_08}
\citation{Liu_16}
\citation{Lima_19}
\citation{Kusakunniran_09}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison among different state-of-the-art gait recognition methods without view variation in all three view angles of CASIA A dataset}}{50}{table.caption.34}}
\newlabel{table:casia_a_result}{{4.1}{50}{Comparison among different state-of-the-art gait recognition methods without view variation in all three view angles of CASIA A dataset}{table.caption.34}{}}
\newlabel{table:casia_a_result@cref}{{[table][1][4]4.1}{[1][50][]50}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Single-View Gait Recognition}{50}{section.4.2}}
\newlabel{sec:single_view}{{4.2}{50}{Single-View Gait Recognition}{section.4.2}{}}
\newlabel{sec:single_view@cref}{{[section][2][4]4.2}{[1][49][]50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Experimental Evaluation on CASIA A dataset}{50}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Experimental Evaluation on CASIA B Dataset}{50}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.1}Experimental Setup}{50}{subsubsection.4.2.2.1}}
\citation{Yu_19}
\citation{Liao_17}
\citation{Liao_19}
\citation{Yu_17_spae}
\citation{Liao_17}
\citation{Liao_17}
\citation{Yu_17_spae}
\citation{Yu_19}
\citation{Liao_19}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Experimental setup for the CASIA B dataset}}{51}{table.caption.35}}
\newlabel{table:caisab_setup}{{4.2}{51}{Experimental setup for the CASIA B dataset}{table.caption.35}{}}
\newlabel{table:caisab_setup@cref}{{[table][2][4]4.2}{[1][51][]51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.2}Results on Single-View Gait Recognition of CASIA B Dataset without View Variation}{51}{subsubsection.4.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.3}Comparison on Single-View Gait Recognition of CASIA B Dataset with State-of-the-art Methods without View Variation}{51}{subsubsection.4.2.2.3}}
\citation{Yu_19}
\citation{Liao_19}
\citation{Yu_17_spae}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Correct class recognition rate (CCR) of the proposed method in all three probe sets of CASIA B dataset}}{52}{table.caption.36}}
\newlabel{table:resutl_without_view}{{4.3}{52}{Correct class recognition rate (CCR) of the proposed method in all three probe sets of CASIA B dataset}{table.caption.36}{}}
\newlabel{table:resutl_without_view@cref}{{[table][3][4]4.3}{[1][51][]52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.4}Results on Single-View Gait Recognition of CASIA B Dataset with View Variation}{52}{subsubsection.4.2.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.5}Comparison on Single-View Gait Recognition of CASIA B Dataset with State-of-the-art Methods with View Variation}{52}{subsubsection.4.2.2.5}}
\citation{Yu_17_spae}
\citation{Yu_19}
\citation{Liao_19}
\citation{Wu_17}
\citation{Kusakunniran_14}
\citation{Kusakunniran_10}
\citation{Wu_17}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Comparison between the proposed method and other state-of-the-art gait recognition methods in CASIA B dataset without view variation}}{53}{table.caption.37}}
\newlabel{table:comp_casia_b_without_view}{{4.4}{53}{Comparison between the proposed method and other state-of-the-art gait recognition methods in CASIA B dataset without view variation}{table.caption.37}{}}
\newlabel{table:comp_casia_b_without_view@cref}{{[table][4][4]4.4}{[1][51][]53}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Cross-View Gait Recognition}{53}{section.4.3}}
\newlabel{sec:cross_view}{{4.3}{53}{Cross-View Gait Recognition}{section.4.3}{}}
\newlabel{sec:cross_view@cref}{{[section][3][4]4.3}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Comparison with the State-of-the-art Methods of CASIA B Dataset on Cross-View Gait Recognition}{53}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Correct class recognition rates (\%) of the proposed method with other state-of-the-art methods on all three probe set of CASIA B dataset without view variation}}{54}{figure.caption.38}}
\newlabel{fig:comp_casia_b_without_view}{{4.3}{54}{Correct class recognition rates (\%) of the proposed method with other state-of-the-art methods on all three probe set of CASIA B dataset without view variation}{figure.caption.38}{}}
\newlabel{fig:comp_casia_b_without_view@cref}{{[figure][3][4]4.3}{[1][51][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison with different state-of-the-art algorithms for gait recognition with view variation in all three probe set of CASIA B dataset}}{54}{figure.caption.41}}
\newlabel{fig:comp_casia_b_with_view}{{4.4}{54}{Comparison with different state-of-the-art algorithms for gait recognition with view variation in all three probe set of CASIA B dataset}{figure.caption.41}{}}
\newlabel{fig:comp_casia_b_with_view@cref}{{[figure][4][4]4.4}{[1][53][]54}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The average recognition rates for all three probe sets of CASIA B dataset. Each row represents the average value of all eleven probe angles at a specific gallery angle ($ \theta _g $) in all three probe sets. \relax }}{55}{table.caption.39}}
\newlabel{table:result_casia_b_with_view}{{4.5}{55}{The average recognition rates for all three probe sets of CASIA B dataset. Each row represents the average value of all eleven probe angles at a specific gallery angle ($ \theta _g $) in all three probe sets. \relax }{table.caption.39}{}}
\newlabel{table:result_casia_b_with_view@cref}{{[table][5][4]4.5}{[1][52][]55}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Comparison among different state-of-the-art methods for gait recognition with view variation in all three probe sets of CASIA B dataset}}{55}{table.caption.40}}
\newlabel{table:comp_casia_b_with_view}{{4.6}{55}{Comparison among different state-of-the-art methods for gait recognition with view variation in all three probe sets of CASIA B dataset}{table.caption.40}{}}
\newlabel{table:comp_casia_b_with_view@cref}{{[table][6][4]4.6}{[1][53][]55}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Multi-View Gait Recognition}{55}{section.4.4}}
\newlabel{sec:multi_view}{{4.4}{55}{Multi-View Gait Recognition}{section.4.4}{}}
\newlabel{sec:multi_view@cref}{{[section][4][4]4.4}{[1][55][]55}}
\citation{Dupuis_13}
\citation{Isaac_17}
\citation{Choudhury_15}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Comparison of our proposed method with the previous best results of cross-view gait recognition }}{56}{table.caption.42}}
\newlabel{table:comp_casia_b_cross_view}{{4.7}{56}{Comparison of our proposed method with the previous best results of cross-view gait recognition}{table.caption.42}{}}
\newlabel{table:comp_casia_b_cross_view@cref}{{[table][7][4]4.7}{[1][53][]56}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces View angle identification rate (\%) of the proposed 3D-CNN network on CASIA B dataset}}{56}{table.caption.43}}
\newlabel{table:result_wd_identification}{{4.8}{56}{View angle identification rate (\%) of the proposed 3D-CNN network on CASIA B dataset}{table.caption.43}{}}
\newlabel{table:result_wd_identification@cref}{{[table][8][4]4.8}{[1][56][]56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Comparison with the State-of-the-art Methods on Multi-View Gait Recognition}{56}{subsection.4.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Comparison with other state-of-the-art methods on all three probe set of CASIA B dataset in multi-view gait recognition}}{57}{table.caption.44}}
\newlabel{table:comp_multi_view}{{4.9}{57}{Comparison with other state-of-the-art methods on all three probe set of CASIA B dataset in multi-view gait recognition}{table.caption.44}{}}
\newlabel{table:comp_multi_view@cref}{{[table][9][4]4.9}{[1][56][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Comparison on average recognition rates (\%) between the proposed method with other state-of-the-art methods in multi-view gait recognition}}{58}{figure.caption.45}}
\newlabel{fig:comp_casia_b_multi_view}{{4.5}{58}{Comparison on average recognition rates (\%) between the proposed method with other state-of-the-art methods in multi-view gait recognition}{figure.caption.45}{}}
\newlabel{fig:comp_casia_b_multi_view@cref}{{[figure][5][4]4.5}{[1][57][]58}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{59}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{5}{59}{Conclusion}{chapter.5}{}}
\newlabel{ch:conclusion@cref}{{[chapter][5][]5}{[1][59][]59}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Summary of Our Work}{59}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Future Prospects of Our Work}{59}{section.5.2}}
\bibstyle{IEEEtran}
\bibdata{buet_msc_thesis}
\bibcite{Boyd_05}{1}
\bibcite{Benabdelkader_02}{2}
\bibcite{Michele_17}{3}
\bibcite{Juen_14}{4}
\bibcite{Lee_14}{5}
\bibcite{karpathy_14}{6}
\bibcite{Cao_19}{7}
\bibcite{Song_17}{8}
\bibcite{Du_15}{9}
\@writefile{toc}{\contentsline {chapter}{\textbf  {Bibliography}}{60}{section.5.2}}
\bibcite{Mao_15}{10}
\bibcite{Yu_06}{11}
\bibcite{Rida_19}{12}
\bibcite{Liu_04}{13}
\bibcite{Han_06}{14}
\bibcite{Bashir_09}{15}
\bibcite{Lam_11}{16}
\bibcite{Wang_12}{17}
\bibcite{Huang_12}{18}
\bibcite{Sarkar_05}{19}
\bibcite{Yam_04}{20}
\bibcite{Gu_10}{21}
\bibcite{Wagg_04}{22}
\bibcite{Pavlovic_99}{23}
\bibcite{Isard_98}{24}
\bibcite{Shutler_04}{25}
\bibcite{Wu_17}{26}
\bibcite{Shiraga_16}{27}
\bibcite{Wolf_16}{28}
\bibcite{Zhang_16}{29}
\bibcite{Yu_17}{30}
\bibcite{Yu_19}{31}
\bibcite{Wei_16}{32}
\bibcite{Cai_16}{33}
\bibcite{Feng_16}{34}
\bibcite{Liao_17}{35}
\bibcite{Liao_19}{36}
\bibcite{Krizhevsky_12}{37}
\bibcite{kal_13}{38}
\bibcite{graves_13}{39}
\bibcite{Tran_15}{40}
\bibcite{Hubel_68}{41}
\bibcite{colah_15}{42}
\bibcite{mikolov_12}{43}
\bibcite{Bengio_94}{44}
\bibcite{Hochreiter_01}{45}
\bibcite{Cho_14}{46}
\bibcite{Chung_14}{47}
\bibcite{Schuster_97}{48}
\bibcite{Graves_08}{49}
\bibcite{Marchand_16}{50}
\bibcite{Ke_10}{51}
\bibcite{Cunado_97}{52}
\bibcite{Wang_04}{53}
\bibcite{Araujo_13}{54}
\bibcite{Ioffe_15}{55}
\bibcite{Graves_05}{56}
\bibcite{Wen_16}{57}
\bibcite{Kingma_15}{58}
\bibcite{keras}{59}
\bibcite{opencv}{60}
\bibcite{Redmon_18}{61}
\bibcite{Hofmann_14}{62}
\bibcite{Noriko_18}{63}
\bibcite{Gross_01}{64}
\bibcite{Wang_03}{65}
\bibcite{Goffredo_08}{66}
\bibcite{Liu_16}{67}
\bibcite{Lima_19}{68}
\bibcite{Kusakunniran_09}{69}
\bibcite{Yu_17_spae}{70}
\bibcite{Kusakunniran_14}{71}
\bibcite{Kusakunniran_10}{72}
\bibcite{Dupuis_13}{73}
\bibcite{Isaac_17}{74}
\bibcite{Choudhury_15}{75}
